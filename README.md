
# AI Autoplayer for Stack

Этот проект представляет **нейросетевого агента**, который автоматически играет в игру *Stack* в реальном времени.  
Агент захватывает изображение с эмулятора, распознаёт позицию плитки и выполняет точные клики в нужный момент.

---

## Суть проекта

- Агент анализирует игровое поле и **распознаёт положение плитки с высокой точностью**.
- Онлайн обучение позволяет **улучшать модель во время игры**, собирая дополнительные данные.
- После финального обучения агент способен **стабильно набирать 500+ очков**, даже если первые несколько кликов намеренно ошибочны.
- Используется самописная библиотека для сетей.

## Демонстрация

![Демонстрация](Data/Images/Demo/demo.gif) 

---
## Используемый эмулятор

- Для запуска игры используется эмулятор **BlueStacks 5**.
- Рекомендуемые настройки:
  - Включить виртуализацию.
  - Внутри настройках эмулятора поставить разрешение: 480×854 и 180 DPI.
  - При использования масштабирования в системе 150%, внутри эмулятора поставить 100% масштабирование;
    При масштабировании в 100% в системе, поставить масштабирование 150% в эмуляторе и т.д.
  - Скорость эмулятора: стандартная
  - Настройка окна: фиксированная позиция на экране (для корректного захвата скриншотов)
- Эмулятор и игра должны быть запущены **до старта агента**.

## Как запустить

1. Запустить и настроить эмулятор.
2. Скачать, установить игру. После установки запустить и купить скин *Platinum* любыми удобными способами. 
   Это обязательно при использовании заранее обученных моделей. 
   При желании можно выбрать любой удобный скин, собрать данные при помощи онлайн обучения и дообучить базовые модели.
3. Проверить работу агента:

   ```bash
   python nn_test.py
   ```

---
## Метрики моделей

| Модель          | Время инференса (мс) | Примечание                    | precison | recall | f1    | accuracy | thr  | Количество параметров |
|-----------------|----------------------| ----------------------------- |----------|--------|-------|----------|------|-----------------------|
| `final_agent`   | 3.5                  | Основная модель               | 0.997    | 0.997  | 0.997 | 0.999    | 0.56 | 1.3M                  |
| `game_over_net` | 1.5                  | Определяет конец игры         | 1.0      | 1.0    | 1.0   | 1.0      | 0.78 | 46K                   |
| `success_net`   | 4.18                 | Проверяет идеальные попадания | 1.0      | 0.999  | 0.999 | 0.999    | 0.95 | 1.6M                  |

> Пайплайн полностью укладывается в 10 мс на кадр, что обеспечивает **реалтайм-игру**.

---
## Требования


- Python 3.9+  
- BlueStacks 5
- При установке cupy обязательно использовать версии совместимые с cuda. Например:

    ```bash
    pip install cupy-cuda12x
    ```

- Проект использует самописную библиотеку для нейросетей, которая **не доступна через pip**  
  Репозиторий библиотеки: [ссылка на репозиторий](https://github.com/Delisseu/NeuralNetwork)  


- Установить зависимости для использования агента:
    ```bash
    pip install -r requirements.txt
    ```
---
## Почему в проекте не использовался test-set

В классических задачах машинного обучения модель оценивают по test-set, однако в задачах игрового агента такое тестирование малоинформативно.
Причина — смещение распределения: датасет фиксирован и отражает только прошлые ситуации, тогда как реальная игра содержит куда более широкий и динамичный спектр состояний.

Чтобы избежать этой проблемы, я использовал онлайн-оценку: модель проверялась непосредственно в игре.
 
Это позволяет:

 * измерять качество на реальных состояниях, а не на замороженной выборке;

 * выявлять провалы модели, которые не встречаются в датасете;

 * гарантировать, что модель оптимизируется под фактическое поведение среды.

Так как целью модели является играть лучше, а не показывать лучший loss на тесте, онлайн-проверка даёт более точную и практически полезную метрику.

---
## Дополнительно

* Для подробностей о сетях, архитектурах и логике онлайн обучения см. [development.md](development.md)